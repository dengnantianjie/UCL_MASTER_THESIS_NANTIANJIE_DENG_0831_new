\chapter{Introduction}
\label{chapterlabel1}

%Some stuff about things.\cite{example-citation} Some more things. 

%Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.
In Chapter~\ref{chapterlabel1} we give a brief introduction of the thesis, which first introduces the models developed before and their applications, and then we propose our Dynamic Author-Topic model as well as explain our collaboration with BBC News Labs where is the data resource from. At last we present our experimental result based on which we are able to illustrate our contributions from this work. 

Topic modeling represents a class of computer programs that mines text, identifies patterns and extracts topics from those texts in an automatic way. The user experience is inputting corpus into a tool which groups words across the corpus into ‘topics’. Miriam Posner \cite{posner2012very} has described topic modeling as a method for finding and tracing clusters of words (called ``topics" in shorthand) in large bodies of texts. One definition of topic is a recurring pattern of co-occurring words, which is put forward in a conference on topic modeling. A topic modeling tool looks through a corpus for these clusters of words and groups them together by a process of similarity. For example, if I feed the computer, say, the last few news of the former Prime Minister Cameron, it will come back that the mainly politics, economy news and recent events  of the prime minister, especially about the briexit. It’s a fairly clever and exceptionally versatile little algorithm that can be customized to all sorts of applications, and a tool that many digital humanists would do well to have in their toolbox.

In 1999 Hofmann \cite{hofmann1999probabilistic} developed probabilistic Latent Semantic Analysis (pLSA) model models information to mine the latent semantic structure of the data, which was put forward in an early stage. Then in 2002 Blei et al. \cite{blei2003latent} described Latent Dirichlet Allocation (LDA) model, which is a special case of topic modeling. It was not the first topic modeling tool, but is by far the most popular, and has enjoyed copious extensions and revisions in the years since. In 2004 Michal and his colleagues proposed Author Topic (AT) Model \cite{rosen2004author} based on LDA, which addressed author information as a parameter. Both LDA and AT model did not consider the time effect among data, therefore the perspective of Time-Aware Topic Model has been put forward. David Blei \cite{blei2006dynamic} and his friends described Dynamic Topic Model (DTM) in 2006. Wang and Andrew \cite{wang2006topics} provided Topic over Time (ToT) model at the same year. And then Tomoharu \cite{iwata2009topic} and his team brought with the Topic Tracking Model (TTM).

LDA is a probabilistic model with interpretable topics. The disadvantages are that it is hard to know when LDA is working, namely, topics are soft-clusters so there is no objective metric to say ``this is the best choice" of hyperparameters. Metrics like perplexity (how well the model explains the data) are okay to test if the learning is working, but very poor indicators of the overall quality of the model. For example, you could have a model with very low perplexity, but whose topics are not very informative. AT model considers each document as a mixture of topics, which includes authorship  information  and  allows  the  mixture  weights  for  different  topics. The disadvantages of AT model are that the similarity among documents addresses with different authors is ignored, and only the topic distribution associated with each author is concerned from the author’s own documents. Besides, both LDA and AT models have not taken time effect, evolution over topics into consideration. DTM, ToT and TTM all play with the document with time-stamp attribution, which means the topic evolution over time has been considered into the models. However, none of them include the author information, which is one of the most important factors related to latent topic generation and correlation between word and topic.

Topic models provide a simple way to analyze large volumes of unlabeled text, therefore this technology has been widely used by the industry in the last more than 10 years. One good example is its application on news, everyday there are 92,000 news articles posted on the web, the number of the news is too overwhelming that it is impossibly large for manual, human processing. It is critical to quickly extract the information from the news corpus, and analyze them to discover the news hot topics underlying them. News is a rolling stream of stories  and is also a valuable resource for academics. However, currently most of the researches on topic modelling are based on static corpus with limited number of articles and not live at all. To our knowledge few literature has really made use of the live stream of news for the purpose of research.

Luckily for our project we are able to access the tremendous live news from BBC News Lab's Jucier API which is still in trial stage and not public. BBC News Labs is the Innovation Programme charged with Driving Innovation for BBC News. News Labs is working with UCL on identifying topics in the News, automatically. This is to support News recommendations, and to do the heavy lifting for Journalists. The project was responsible by Dr. Sylvia Tippmann and other BBC news researchers. The API key of the Juicer which is a news aggregation and content extraction API is kindly provided to UCL for doing experiments. We hereby thanks for BBC news lab's generous investment in our project.

We have noticed the hierarchical structure of the news corpus, since on the one hand a BBC news has a category label which is defined by the editor, such as \textit{UK, politics, education, health}, etc, which can be regarded as the virtual authors, on the other hand, we assume that there is a underlying semantic relations among the news which are the  topics. Therefore by using Author-Topic model we are able to simultaneously model the content of the document and also the patterns
of the virtual authors. At the same time as an extension, news  is a continuously streaming of a sequence
of documents that its topic distribution keeps changing over time with previously salient topics fading-off. We can regard the news as a discrete stream where in a certain time range, such as a week, the topic and author distribution remain static, but with the new documents coming in at the next time frame the distributions and hyperparameters will change accordingly. The inference we obtained from the current time frame can be used as an evidence for the inference of next one, thus we can capture the trending of the news and track the evolution of the topics. 

Therefore in this paper we propose the Dynamic Author-Topic (DAT) model which draws upon the strength of Author-Topic model and Dynamic Topic model that is used for modelling temporal information of the document corpus. We try to solve the three problems in this paper, which are \textbf{Tagging}, meaning based on the content and time of the news we are able to automatically
tag the news with an category, and \textbf{Summarization}, meaning based on the author-topic distribution we are able to know what
topics are most discussed in one type of news
 and also \textbf{Dynamics},  to see that whether we are able to monitor the changes of event of interest for the
news over time. We have modelled our algorithm as a generative model and the inference is done by Gibbs sampling. We have deduced the update rules for the parameters of our model and implemented accordingly. We also employ the algorithms of the three state-of-art topic models, which are Latent Dirichlet Allocation (LDA) model, Author Topic (AT) model and Topic Tracking model (TTM). All the  implementation are done on the same platform and in the same environment, to make sure that our experimental results are valid.

Our target is to evaluate the performance of DAT model in terms of topic extraction, topic evolution, and topic generalization, we also measure the performance difference of DAT model in terms of different length of time dependency and topic numbers. It turns out that DAT model outperforms the other three algorithms, especially for topic evolution and topic extraction, since DAT is proven to be able to find the time-related and diverse news which are exactly the hot topics for a certain time. 

In summary, the contributions of this paper are as follows:
\begin{itemize}
    \item We propose a Dynamic Author-Topic model that captures the temporal information of news stream and tracks the author and topic distribution changing over time. We present the inference process of the model using Gibbs sampling for the topic and author probability distribution and its update rules whicn can be implemented for our algorithm.
    \item  We have successfully implemented the whole experimental framework using C++ and Python, not only for DAT model, but also for LDA, AT, and TTM. To eliminate the bias of experimental results we employ all the four algorithms based on the same code base and running on the same platform. We have also done a comprehensive research for most of the state-of-art topic modelling algorithm and compare the performance of DAT with three classic ones. Therefore we have the confidence to claim that DAT model works better in terms of topic extraction and topic tracking over time.
    \item We have analyzed the effectiveness of the proposed DAT model by measuring in the dimension of topic extraction, topic evolution and topic generalization, and demonstrated that our model outperforms the other state-of-art models.
    \item We have made use of the real-world news stream data which is provided by BBC News Lab, to the best of our knowledge only a few of the literature have really worked on the real streaming dataset.  
\end{itemize}

The rest of the paper is organised as follows, Section 2 discusses the related work, our model description and inference of DAT model is formulated in Section 3 and in Section 4 we present the experimental setup, data preprocessing and introduce the algorithms for the three baselines. The experimental results are shown in Section 5 followed by conclusion and future work in the last section.


\chapter{Introduction}
\label{chapterlabel1}

%Some stuff about things.\cite{example-citation} Some more things. 

%Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.

Topic modeling represents a class of computer programs that mines text, identifies patterns and extracts topics from those texts in an automatic way. The user experience is inputting corpus into a tool which groups words across the corpus into ‘topics’. Miriam Posner \cite{posner2012very} has described topic modeling as a method for finding and tracing clusters of words (called ``topics" in shorthand) in large bodies of texts.One definition of topic is a recurring pattern of co-occurring words, which is put forward in a conference on topic modeling. A topic modeling tool looks through a corpus for these clusters of words and groups them together by a process of similarity. For example, if I feed the computer, say, the last few news of the former Prime Minister Cameron, it will come back that the mainly politics, economy news and recent events  of the prime minister, especially about the briexit. It’s a fairly clever and exceptionally versatile little algorithm that can be customized to all sorts of applications, and a tool that many digital humanists would do well to have in their toolbox.

In 1999 Hofmann \cite{hofmann1999probabilistic} developed Probabilistic Latent Semantic Analysis (pLSA) model models information to mine the latent semantic structure of the data, which was put forward in an early stage. Then in 2002 Blei et al. \cite{blei2003latent} described Latent Dirichlet Allocation (LDA) model, which is a special case of topic modeling. It was not the first topic modeling tool, but is by far the most popular, and has enjoyed copious extensions and revisions in the years since. In 2004 Michal and his colleagues proposed Author Topic (AT) Model \cite{rosen2004author} based on LDA, which addressed author information as a parameter. Both LDA and AT model did not consider the time effect among data, therefore the perspective of Time-Aware Topic Model has been put forward. David Blei \cite{blei2006dynamic} and his friends described Dynamic Topic Model (DTM) in 2006. Wang and Andrew \cite{wang2006topics} provided Topic over Time (ToT) model at the same year. And then Tomoharu \cite{iwata2009topic} and his team brought with the Topic Tracking Model (TTM).

LDA is a probabilistic model with interpretable topics. The disadvantages are that it is hard to know when LDA is working, namely, topics are soft-clusters so there is no objective metric to say ``this is the best choice" of hyperparameters. Metrics like perplexity (how well the model explains the data) are okay to test if the learning is working, but very poor indicators of the overall quality of the model. For example, you could have a model with very low perplexity, but whose topics are not very informative. AT model considers each document as a mixture of topics, which includes authorship  information  and  allows  the  mixture  weights  for  different  topics. The disadvantage of AT model are that the similarity among documents addresses with different authors is ignored, and only the topic distribution associated with each author is concerned from the author’s own documents. Besides, both LDA and AT models have not taken time effect, evolution over topics into consideration. DTM, ToT and TTM all play with the document with time-stamp attribution, which means the topic evolution over time has been considered into the models. However, none of them include the author information, which is one of the most important factors related to latent topic generation and correlation between word and topic.



/////////BBC 
BBC News Labs is the Innovation Programme charged with Driving Innovation for BBC News. News Labs is working with UCL on identifying topics in the News, automatically. This is to support News recommendations, and to do the heavy lifting for Journalists. The project was responsible by Dr. Sylvia Tippmann and other BBC news researchers. The API key of the Juicer which is a news aggregation and content extraction API is kindly provided to UCL for doing experiments. We hereby thanks for BBC news' lab's generous investment in our project !


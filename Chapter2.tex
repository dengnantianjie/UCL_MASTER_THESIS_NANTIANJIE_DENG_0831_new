\chapter{Related Work}
\label{chapterlabel2}

There are two models related to our work, static topic model and time-aware topic model with a rich literature and corresponding applications available on those topics. In the following sections we introduce each model. 

\section{Static Topic Model}
In recent years it becomes more difficult to search and discover the pinpoint information when more knowledge and information become available to people. The tool which can help us to search,classify and comprehend the great amount of information is needed. 

Static topic model is the way to help people automatically searching, organizing, understanding the vast amounts of information and electronic documents. By taking several collections of documents and using a series of algorithms, a set of latent hidden topics can be inferred  and discovered by topic modeling. Besides, the annotation information of to what extent each document contributes to those latent topics can be obtained, which bridges the documents and topics. People can use the annotations information to understand, classify, summarize and search the archives.

\subsection{Probabilistic Latent Semantic Analysis Model}

Probabilistic Latent Semantic Analysis (pLSA) was developed by Hofmann \cite{hofmann1999probabilistic} in 1999, and was widely applied on text-based applications, such as clustering, retrieval and retrieval. In terms of the category of topic models, concurrence information is modeled by pLSA in the condition of a probabilistic framework, which can be used to capture the latent semantic structure of the data. PLSA can be modeled in two ways: latent variable model based on a statistical model, and matrix factorization for sparse co-occurrence matrix.

Then in 2001 Hofmann \cite{hofmann2001unsupervised} provided an innovative process to learn pLSA in an unsupervised way. A generative latent class model is used  to operate a probabilistic mixture decomposition, which leads to a more precise performance with a solid foundation in statistical inference. 

\subsection{Latent Dirichlet Allocation Model}\label{lda}

Blei et al. in \cite{blei2003latent} firstly proposed a generative model that models each document as a mixture of topics, each topic as a distribution over words and each word as derived from topics. In fact, only documents can be observed and other variables are hidden variables. Therefore inferring the hidden variables, for example, compute the distribution of topics, proportions and assignments conditioned on the documents is critical. In LDA, each document can be regarded as formed by the same shared topics but with different proportions. The hidden thematic structure can be visualized and then new data is generalized to fit into the structure by using LDA. 

Numerous researches have been extended based on Blei's work in 2003. LDA \cite{blei2003latent} model outperforms than other models because it holds the idea that documents are comprised of words drawn from several topics, instead of a singe topic, which obviously increased the computational cost of unsupervised estimation especially when people can only observe the words and have no hidden corresponding topics off hand. 

Hoffman et al. \cite{hoffman2010online} provided a much faster online algorithm that can process a great amount of documents and can converge streaming collections of text for Latent Dirichlet Allocation. Anandkumar et al. \cite{anandkumar2012spectral} proposed a more simple and efficient learning algorithm, a spectral algorithm, which guarantees to find the parameters for Latent Dirichlet Allocation. Krestel et al. \cite{krestel2009latent} introduced a method which has a rapid improvement of the speed of LDA Gibbs sampling by taking advantage of organizing the computations in a better way, estimating a dynamic upper bound in an adaptive way and providing precisely equivalent samples.

TWILITE, developed by Kim et al. \cite{kim2014twilite} using LDA model, is a recommendation system which can return the top tweets and provide to the user as well as the top users that can be followed by the certain user. When people tweet messages or add new friends, this posting process can be captured by TWILITE by constructing an latent Dirichlet allocation model and this process of network connection can be obtained by using matrix factorization. The new model can obtain the hidden corresponding topics  by analyzing both the contents of tweet messages and network relationships  and recommend to users, with faster speed and better performance.

\subsection{Author Topic Model}\label{2at}

Rosen et al. \cite{rosen2004author} and Steyvers et al. \cite{steyvers2004probabilistic} described the Author-Topic (AT) model, a generative model for document collections, which models each document as a mixture of topics, like Latent Dirichlet Allocation \cite{blei2003latent}. Including authorship information and allowing the mixture weights for different topics to be determined by the authors of the document is the extension based on topic models. The content of documents and the interests of authors can be simultaneously modeled by AT model, which hold an idea that a probability distribution over topics is deemed to represent each author,  a probability distribution over words for that topic is deemed to represent each topic. From a mixture of each authors’ topic, the words are regarded as the result in a multi-author paper. A document with multiple authors can be regarded as a distribution over topics which is a mixture of the distributions associated with the authors. By learning the parameters of the innovative model, the relationships between authors, documents, topics, and words can be explored.

Numerous researches have been extended based on Author-Topic (AT) model. McCallum et al. \cite{mccallum2005author} proposed an Author-Recipient-Topic (ART) model for networking message data, which the latent topics and the directed senders and receivers in social network can be captured  distinctly on both the author and one recipient of a message. The difference between AT and ART model is that the ART model considers not only author but also recipients, regarding experiments with Enron and Academic Email as mixture of topics. ART model is useful when processing with large bodies of networking message data, with better performance in finding topics conditioned on message sending relationships,  exploring in relationship of social roles and understanding the content of message data in order to make recommendations and prioritization.

Other extension based on Author-Topic (AT) model is a recommendation system for users. Rosen et al. \cite{rosen2010learning} applied the author-topic model on large text corpora, including papers, abstracts and emails. By analyzing the words expressed in a paper, latent topics extracting from documents and the names of the authors as well as utilizing the result of AT model, automatic recommendation system can be obtained to push recommendations with related topics and authors. Jiang et al. \cite{jiang2015author} proposed the personalized traveling recommendation system based on Author Topic Model using Collaborative Filtering (ATCF) method. By processing and analyzing the profile of user and travel habit, the latent travel topics and travel preference can be obtained simultaneously and then make customized recommendations to users.

\section{Time-Aware Topic Model}

The effect of time is not considered by static topic model, which is not adaptive for many evolution applications. As we know, a list of recent topics usually sorted by popularity or freshness.  For example, the search volume of the topic of Apple always increased abruptly when Apple Inc releases the new products. Obviously, the time factor is critical and should be considered. Therefore time-aware topic model came into being, which models topics concerning time variation. In the following sections we introduce three models related to time-aware topic model.

\subsection{Dynamic Topic Model}

Dynamic Topic Model (DTM) \cite{blei2006dynamic} captures the natural parameters in state space model, which models sequential topics drawn from the organized collections of documents in evolving way and takes advantage of Kalman filters and wavelet regression as variational approximations. DTM requires discretized time slices, namely, documents are classified by a period of time, for example, month by month, under which model a series of topics evolved from last month’s topics are arisen from each month’s documents. Therefore determining the length of the time span influences the performance of the Dynamic Topic Model. Iwata et al. \cite{iwata2009topic} developed the method that tracks the topic changes by multi-scale time span to solve the problem. Dynamic Topic Model provides an innovative way when processing unstructured data in large amount.

Wang et al. \cite{wang2012continuous} developed the continuous time Dynamic Topic Model (cDTM), which applies Brownian motion that is continuous generalization on a sequential collection of documents to model continuous-time topic evolution. It is noteworthy that the pattern of the topic can be extracted from the word which evolves over the collection of documents. Compared with DMT, the advantage of the cDMT is that time can be continuous and we can apply sparse variational inference to compare models in faster speed. 

Normally, we think that documents are the mixture of topics and we use the whole document level to compute the mixture, which means the topics are extracted from the entire collection of document with no need to specify where the topics occur in the document. Canny et al \cite{canny2006dynamic} described a new model which refers every word in the whole collection of document to compute the topic mixture. John brought the concept of topical segmentation in dynamic topic estimation that a document can be broken into several passages which contain only single topic and are different with adjacent passages. By taking estimation of per-word topic mixture distribution, John extended the dynamic topic model on document segmentation.

Iwata et al. \cite{iwata2010online} described an online dynamic topic model with multi scale, which can extract the latent topics from the collections of documents in sequential and evolving way. In real word, the way topics evolve relates with multiple timescales. For instance, some network glossaries just appear and disappear in very short time with the short-timescale dependency, while other words can be spread and used over hundred years with the long-timescale dependency. Therefore the distribution of the of the current topic over words are related with the generation of the distribution of words on the multi scale in the previous epoch. The evolution of topics at various resolutions of timescales can be analyzed by the use of multiscale dynamic topic model, which has a better performance both in effectiveness and computational efficiency in collections of the documents with timestamps.

\subsection{Topic over Time Model}\label{2tot}
Drawn from the LDA topic model, Wang et al. \cite{wang2006topics} provided Topic over Time (ToT) model, which concerns not only the structure of data, but also the variation of the structure over times. The difference between ToT and DTM is that topics are related with a continuous and discrete distribution over timestamps respectively, where the discretization results from associating with each topic a continuous distribution over time can be avoided. Both word co-occurrences and temporal information have an influence on topic generation, where the timestamp values plays an important role in ToT model. The advantage of ToT is that the prediction of absolute time values when given an unstamped document and topic distributions given a timestamp can be done in long-range dependency, as well as deducing the risk of improper division in one topic where has a gap in its appearance by the use of Markov model.

Extended from ToT model, Dubey et al. \cite{dubey2013nonparametric} described a nonparametric Topics over Time (npToT) model, which incorporates with time-varying topics. Unlike the ToT model, the first distinction of npToT is that an unbounded number of topics is allowed, which can reach the peak of popularity in an unbounded number of times. The second distinction of npToT is that the correlations between the time changes in topic popularity is induced, where the flexible distribution over the temporal changes over the topics’ popularity can be captured with relative low computational complexity. Therefore, in npToT model the related topics performance in similar manners. Timestamp can be regarded as a random variable when combining it with text for document jointly computation by the use of ToT model. In that way, non-Markovian dynamics can be incorporated with reasonable inference requirements, namely, the data can be processed without covariate information within the framework. In npToT model, the pairs of text and time, document and timestamp can be regarded as exchangeable variable, which can be used when constructing a Gibbs sampling scheme.

Twitter is the most widely used social media, where the millions of real-time tweets are published. In order to analyze and visualize a high-level overview of a Twitter stream, Malik et al. \cite{malik2013topicflow} applied real-time twitter data on binned topic model (statistical topic modeling and alignment) where automatically generated topics and topic flow can be organized by related twitter data. The ``topics" that extracted from a collection of documents can be discovered by statistical topic modeling. The binned topic model, an innovative extension from statistical topic modeling, concerns the variation in topics over time and identifies the appearance of fresh topics, where the deeper insight from the data can be analyzed from the combination of the topics. The natural attribution of twitter data is continuously evolving and diverse, which can be simulated by binned topic model. The distinction of the topics generated from binned topic model is that adjacent time slice of data does not directly correspond to the other topics, with the next step of alignment. The emergence, convergence, and divergence of complex topics in a twitter stream can be visualized by topic flow.

Another extension such as Thomas et al. \cite{thomas2010validating} used the topic model to analyze the evolution of software, and found that spikes and drops in the metric values of topics can be characterized by the topic evolutions. Hall et al. \cite{hall2008studying} studied about the development of ideas in a scientific field by the use of topic models, which found the trend of rise, steady increase and sharp decline and verified the strength of each topic in different time period.

\subsection{Topic Tracking Model}

Iwata et al. \cite{iwata2009topic} brought up Topic Tracking Model (TTM), which can track the time-varying consumer purchase behavior for analyzing the interests of individual consumers and item trends change in each topic over time. The adaptive changes of interests for consumers and trends for topics and trends can be tracked by TTM based on current purchase records and previously estimated interests, namely, the current inferences of customer purchase behavior can be captured without the past data, which decreases the computational cost and the memory requirement. Compared with other models, the advantage of TTM is that it can model the variations in an adaptive way, because dynamics of both interests and trends can be modeled and the persistency for each time interval from the given data can be inferred.

Zhang et al. \cite{zhang2010topic} extended the dynamic topic model for tracked topic evolving and proposed a new method, topic-based TF-IDF weighting method for topic tracking and topical importance of the features testing. The improvement of the paper’s method is that the topic drift problem is overcome and the noise existing in the tracked topic description is filtered, which improve the performance of topic tracking significantly. The update rule of the model is that the topic model is extended by the use of the incoming related stories and the noise in the topic model is filtered by the use of the incoming unrelated stories.

Many extensions have been developed based on Topic Tracking Model (TTM). Yincheng et al. \cite{yincheng2014adaptive} explored a deeper insight of adaptive topic tracking model based on title semantic domain topic model and double-state strategy. The title-centric semantic domain cohesion of reports can be enhanced and dimensions of reports’ feature space effectively can be reduced by the title semantic domain topic model. The main idea of the double-state strategy is that by the use of the combination of static model and dynamic model, where the difference of the models is a given number of training reports for construction of the topic model is used for static model and the sliding text window mechanism for capture of the new contents of a topic, removing outdated topics and reflecting the changes of topic’s focus in a timely manner is used for dynamic model. The performance of the adaptive topic tracking model is improved in some extent when applying the two strategies into TTM. 

Lee et al. \cite{lee2008news} explored a new keyword extraction technique which can be used for topic tracking model. As we know, the high-level description of article’s contents can be given by a set of keywords, therefore, a short summary of news articles can be obtained by identifying keywords from collections of documents properly. The main idea of the technique consists of two stages: extracting candidate keywords from a given document set and removing meaningless words by comparing candidate keywords in terms of ranking results. The variants of the conventional TF-IDF model and filtering keywords with cross-domain comparison are considered to increase the efficiency and accuracy for topic tracking system. Compared with manual keyword extraction, the problem of processing difficulty and time consuming is solved.

Other examples such as Watanabe et al. \cite{watanabe2011topic} described a topic tracking language model, which can accommodate the changes of speakers, speaking styles and topics. The variations of topics based on current text information and previously estimated topic models can be tracked adaptively. Mulbregt et al. \cite{van1998text} applied a Hidden Markov Models and language modeling techniques on text segmentation and topic tracking, which has a better performance in inferring story boundaries and retrieving stories related to a specific topic.

\section{Applications}

In this section, we introduce the applications of topic models. Specifically, Section~\ref{subsec:plsaApplication} introduces the applications of pattern recognition and personalized web engine based on Probabilistic Latent Semantic Analysis (pLSA) model. Section~\ref{subsec:ldaApplication} introduces the applications based on Latent Dirichlet Allocation (LDA) model for text, image and audio processing; Section~\ref{subsec:atmApplication} introduces the applications based on Author Topic (AT) model for data mining; Section~\ref{subsec:dtmApplication} introduces the applications based on Dynamic Topic Model (DTM) for data monitoring; Section~\ref{subsec:totApplication} introduce the application based on Topic over Time (ToT) model with author information ; Section~\ref{subsec:ttmApplication} introduces the applications based on Topic Tracking Model (TTM) for news tracking.

\subsection{Probabilistic Latent Semantic Analysis Model}
\label{subsec:plsaApplication}

Li et al. \cite{li2008global} applied probabilistic Latent Semantic Analysis (pLSA) model on pattern recognition of global behavior, which can inference global behavior and detect anomaly both locally and globally in a wide-area scene. The scene is decomposed into the preinstalled regions and behavior is unscrambled into characteristics.

Lin et al. \cite{lin2005using} provided a personalized web search based on pLSA model, where the latent useful information on the Internet can be quickly obtained under the consideration of web behaviors. In that way, the unseen factors are mined  and technique is analyzed to construct  the customized web search.

Monay et al. \cite{monay2004plsa} applied pLSA model to solve the problem of unsupervised image auto-annotation under the probabilistic latent space models. Multi-modal co-occurrences are modeled and the definition of the latent space is constrained, which guarantees the consistency in latent words and ability to gain visual information.

\subsection{Latent Dirichlet Allocation Model}
\label{subsec:ldaApplication}

Haruechaiyasak et al. \cite{haruechaiyasak2008article} applied the Latent Dirichlet Allocation (LDA) model for recommendation of related articles for Wikipedia selection for schools. Each article can be regarded as a probability distribution on the topic model and similarity among the articles’ topic distribution profiles can be calculated. The experimental results showed that the related articles can be discovered, both within and without in hyperlinks. 

Ramage et al. \cite{ramage2009labeled} demonstrated Labeled LDA Model, in which a one-to-one correspondence between LDA’s latent topics and user tags is defined with constrain of the LDA model. By learning word-tag correspondences directly, the problem of credit attribution in multi-labeled corpora can be solved.

Kim et al. \cite{kim2009acoustic} extended Latent Dirichlet Allocation (LDA) algorithm on an acoustic topic model, in which the related hidden acoustic topics can be captured in a given audio signal in an unsupervised way. The introduction of the notion of acoustic words in audio document supports the LDA model in audio framework.

Liu et al. \cite{liu2012attribute} provided an innovative Attribute Restricted Latent Topic Model (ARLTM) for person re-identification, which can encode targets into semantic topics. ARLTM has better performance because it imposes human-specific attributes as semantic restriction priors in a principled generative process.

Zhai et al. \cite{zhai2011constrained} applied Latent Dirichlet Allocation (LDA) Model on the data of product reviews with the ability to process large scale constraints. As we know, the expression and opinion of product is subjective, therefore the idea that the words and phrases can be grouped under the same feature is useful for obtaining an effective summary. By the application of the constrained-LDA and the extracted constraints, product feature can be group in an effective way.

\subsection{Author Topic Model}
\label{subsec:atmApplication}

Tsai et al. \cite{tsai2011tag} proposed a tag-topic model for mining information from blog data, which referenced from Author-Topic (AT) model. The attribute of blog data is slightly different with text documents, therefore Flora brought the idea that blog tags and words, the semantic annotations in a given topic in a collection of blog posts, can be regarded as the valuable information of additional labels for the myriad of blog documents, which shares similarities to author label in Author-Topic model. 

Shu et al. \cite{shu2009latent} addressed author-topic model and LDA model into solving the three kinds of entity-name problems, which includes name sharing problem, name variant problem and name mixing problem. By taking the information of words and author names into consideration and proposing a method for model parameters capture, the mentioned problem can be solved by complete entity resolution.

Xu et al. \cite{xu2011discovering} proposed an innovative framework, a modified author-topic model to solve the problem of discovering users’ topics of interest on Twitter. The highlights of the work is that the noisy posts and unrelated networking connection are excluded by the use of a latent variable to identify whether it is related to its author’s interest.

McCallum et al. \cite{mccallum2005topic} proposed the Author Recipient-Topic (ART) model based on Author-Topic (AT) model, which can model the language content and topics and obtain the topic distributions over the direction-sensitive messages sent between entities for social network analysis. For example, an email has both the sender and recipient, by analyzing the content and relationship between sender and recipient, the discovery of topic and role can be steered.

Dai et al. \cite{dai2011grouped} described a generative model, Grouped Author Topic Model, which can be used for processing the problem of entity resolution without fixed assumption of the number of identities under unsupervised way. Tackling with the phenomenon that one author may associate with difference references, Andrew addressed authors and topics that related with latent groups. The generation of an abstract and an author list is from a given group in each document.

\subsection{Dynamic Topic Model}
\label{subsec:dtmApplication}

Zhang et al. \cite{zhang2015dynamic} applied a dynamic topic model on real-time monitoring system for market competition from online text and image data, which can provide companies with reports of highly overlapped or discriminative topics with their major competitors. The goal of the application is the latent topics (e.g. shoes, bags, clothes) can be automatically detected, the market competition shared by different brands (e.g. Chanel, Gucci, Prada) can be analyzed and the temporal evolution of the brands' stakes over the shared topics can be tracked.

Hospedales et al. \cite{hospedales2012video} proposed a Markov Clustering Topic Model (MCTM) improved from dynamic topic model, which can be used to automatically process with public space video data, under contemporary commercial and security considerations. MCTM overcome the challenges of profiling complex object behavior and analyzing under the conditions of visual occlusions and ambiguities by modeling dynamic scenes into visual events and visual events into global behaviors.

Derntl et al. \cite{derntl2013dynamic} applied dynamic topic model on the Learning Analytics and Knowledge (LAK) data, which has the challenges in visual analytics of topic dynamics perspective. DVITA a web-based browsing tool, is developed and deployed to explore and visualize the results from dynamic topic models. 


\subsection{Topic over Time Model}
\label{subsec:totApplication}

Xu et al. \cite{xu2014author} proposed Author-Topic over Time (AToT) model, which extended Topic over Time (ToT) model with author information and inferred model parameters by the use of collapsed Gibbs sampling method. Not only the latent topics and users’ interests can be discovered by AToT, but also the the evolution pattern of them can be analyzed.

\subsection{Topic Tracking Model}
\label{subsec:ttmApplication}

Ha et al. \cite{ha2009relevance} provided a relevance-based topic model extended from Topic Tracking Model (TTM), which can overcome the limitations of scalability and inability to rule out non-relevant portions in text streams. This framework has a better efficient performance in discovering temporal patterns  since news events are tracked hierarchically under different levels of granularity.

Yamron et al. \cite{yamron1999topic} applied topic tracking system on news stream, which is based on Topic Detection and Tracking (TDT) Evaluation. The standard language modeling techniques are used for document similarity measurement with default test conditions: tracking in newswire, recognized broadcast and story samples, which can improve the smoothness, discrimination and adaptation.

Jin et al. \cite{jin1999topic} applied topic tracking model on multiple sources of data in the form of radio, TV broadcast and newswire. The main challenge of score normalization across topics is overcome based on probabilistic tracking system, which obtains a better performance in efficiency and effectiveness. 



% This just dumps some pseudolatin in so you can see some text in place.

 